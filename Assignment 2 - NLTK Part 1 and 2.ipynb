{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f20a330a-9503-42f4-8e21-f840e3597a06",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "### Basic Tokenization Process \n",
    "1. Extract Text\n",
    "2. Generate tokens (sentences --> words)\n",
    "3. Remove stop words, non-characters, punctuations; convert to lowercase \n",
    "4. Check Stem words\n",
    "5. Check frequencies\n",
    "6. Check word combinations (parts of speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49c7ddb-fce8-470b-876e-3a82956cf248",
   "metadata": {},
   "source": [
    "### Install NLTK and necessary resources\n",
    "<font color=blue> **names:**</font> list of common English names <br>\n",
    "<font color=blue> **stopwords:**</font> list of common words, articles, pronounes, prepositions, and conjunctions <br>\n",
    "<font color=blue> **vader_lexicon:**</font> list of woreds and jargons referenced during sentiment analysis <br>\n",
    "<font color=blue> **averaged_perceptron_tagger:**</font> data model used to categorize words into parts of speech <br>\n",
    "<font color=blue> **punkt:**</font> data model that splits full texts into word lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0f63378-1439-4bfe-a753-c7be037f3be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/BGBlanco/opt/anaconda3/lib/python3.8/site-packages (3.6.1)\n",
      "Requirement already satisfied: click in /Users/BGBlanco/opt/anaconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in /Users/BGBlanco/opt/anaconda3/lib/python3.8/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: regex in /Users/BGBlanco/opt/anaconda3/lib/python3.8/site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: tqdm in /Users/BGBlanco/opt/anaconda3/lib/python3.8/site-packages (from nltk) (4.59.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to /Users/BGBlanco/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/BGBlanco/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/BGBlanco/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/BGBlanco/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/BGBlanco/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download([\n",
    "\"names\",\n",
    "\"stopwords\",\n",
    "\"averaged_perceptron_tagger\",\n",
    "\"vader_lexicon\",\n",
    "\"punkt\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6221a6-76b9-4076-aca5-7e54bfc572d5",
   "metadata": {},
   "source": [
    "## Martin Luther King's \"I have a dream\" speech\n",
    "### Step 1: Load and Extract Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3674fda0-05e4-45bc-836d-c4503fc46096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text\n",
    "filename = 'I have a dream excerpt.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eb5499-a35f-4143-bb6c-c18ffeb7face",
   "metadata": {},
   "source": [
    "### Step 2: Generate tokens - sentences and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9926f8d6-e020-40d4-b075-281e3888ded3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So even though we face the difficulties of today and tomorrow, I still have a dream.\n"
     ]
    }
   ],
   "source": [
    "# split into sentences\n",
    "from nltk import sent_tokenize\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences[0])  # print 1st sentence with [0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1da46d1f-5cad-48a9-9c2c-3efaf0913b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a dream that one day this nation will rise up and live out the true meaning of its creed: We hold these truths to be self-evident, that all men are created equal.\n"
     ]
    }
   ],
   "source": [
    "print(sentences[2])  # print 3rd sentence with [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78a2d9de-9275-41da-bfdf-a609a52eef8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['So', 'even', 'though', 'we', 'face', 'the', 'difficulties', 'of', 'today', 'and', 'tomorrow', ',', 'I', 'still', 'have', 'a', 'dream', '.', 'It', 'is', 'a', 'dream', 'deeply', 'rooted', 'in', 'the', 'American', 'dream', '.', 'I', 'have', 'a', 'dream', 'that', 'one', 'day', 'this', 'nation', 'will', 'rise', 'up', 'and', 'live', 'out', 'the', 'true', 'meaning', 'of', 'its', 'creed', ':', 'We', 'hold', 'these', 'truths', 'to', 'be', 'self-evident', ',', 'that', 'all', 'men', 'are', 'created', 'equal']\n"
     ]
    }
   ],
   "source": [
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens[:65]) # print the first 65 words "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05988714-8ef5-4934-a1a2-f91ffcbdef15",
   "metadata": {},
   "source": [
    "### Step 3: Remove stop words, non-characters, punctuations; convert to lowercase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f11ffc9-0348-45f0-b645-e7e2d4a92c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['So', 'even', 'though', 'we', 'face', 'the', 'difficulties', 'of', 'today', 'and', 'tomorrow']\n"
     ]
    }
   ],
   "source": [
    "# extract alpha only\n",
    "words = [word for word in tokens if word.isalpha()]\n",
    "print(words[:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fc2f668-0881-4bec-97fb-ff3b4ce513eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "# use stopwords resource\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e3c445f-4214-4209-8cbc-d5fadd83a9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['so', 'even', 'though', 'we', 'face', 'the', 'difficulties', 'of', 'today', 'and', 'tomorrow', ',', 'i', 'still', 'have', 'a', 'dream', '.', 'it', 'is', 'a', 'dream', 'deeply', 'rooted', 'in', 'the', 'american', 'dream', '.', 'i', 'have', 'a', 'dream', 'that', 'one', 'day', 'this', 'nation', 'will', 'rise', 'up', 'and', 'live', 'out', 'the', 'true', 'meaning', 'of', 'its', 'creed', ':', 'we', 'hold', 'these', 'truths', 'to', 'be', 'self-evident', ',', 'that', 'all', 'men', 'are', 'created', 'equal']\n"
     ]
    }
   ],
   "source": [
    "# convert to lower case\n",
    "tokens = [w.lower()for w in tokens]\n",
    "print(tokens[:65])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cac6b8b-e7ec-4d33-8bb1-ee9de7843322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# check punctuations\n",
    "import string\n",
    "print (string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24969960-783b-40fe-945a-f074a1bb2391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['so', 'even', 'though', 'we', 'face', 'the', 'difficulties', 'of', 'today', 'and', 'tomorrow', '']\n"
     ]
    }
   ],
   "source": [
    "# remove punctuations from lower case set\n",
    "table = str.maketrans('','',string.punctuation) #maketrans maps characters to replace\n",
    "stripped = [w.translate(table) for w in tokens] #translate usually with maketrans replaces characters per mapping table\n",
    "print(stripped[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ca19ce6-813d-48b8-9a2d-4dd0324d2b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['so', 'even', 'though', 'we', 'face', 'the', 'difficulties', 'of', 'today', 'and', 'tomorrow', 'i', 'still', 'have', 'a', 'dream', 'it', 'is', 'a', 'dream', 'deeply', 'rooted', 'in', 'the', 'american', 'dream', 'i', 'have', 'a', 'dream', 'that', 'one', 'day', 'this', 'nation', 'will', 'rise', 'up', 'and', 'live', 'out', 'the', 'true', 'meaning', 'of', 'its', 'creed', 'we', 'hold', 'these', 'truths', 'to', 'be', 'selfevident', 'that', 'all', 'men', 'are', 'created', 'equal']\n"
     ]
    }
   ],
   "source": [
    "# remove non-alpha tokens after removing punctuations\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "print(words[:60]) # from 65 words to only 60 without punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dab0f7d2-6081-4b5c-9a8e-e9c813060d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['even', 'though', 'face', 'difficulties', 'today', 'tomorrow', 'still', 'dream', 'dream', 'deeply', 'rooted', 'american', 'dream', 'dream', 'one', 'day', 'nation', 'rise', 'live', 'true', 'meaning', 'creed', 'hold', 'truths', 'selfevident', 'men', 'created', 'equal']\n"
     ]
    }
   ],
   "source": [
    "# filter out stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words_filtered = [w for w in words if not w in stop_words] # new text without stop words\n",
    "print(words_filtered[:28])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1b88aa-3d74-4926-bc1f-76fd56c25222",
   "metadata": {},
   "source": [
    "### Step 4: Check stem words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcb693a4-baa2-4e06-8442-617a5ca0cfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['even', 'though', 'face', 'difficulti', 'today', 'tomorrow', 'still', 'dream', 'dream', 'deepli', 'root', 'american', 'dream', 'dream', 'one', 'day', 'nation', 'rise', 'live', 'true']\n"
     ]
    }
   ],
   "source": [
    "# stemming words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in words_filtered]\n",
    "print(stemmed[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdc4593-dc47-49cc-8f78-d18800e73e1c",
   "metadata": {},
   "source": [
    "### Step 5: Check frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34f9d82a-c120-4f76-bf83-e8977ed36eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " freedom     ring    dream      let      day    every      one     able together   nation \n",
      "      13       12       11       11       10        9        8        8        7        4 \n"
     ]
    }
   ],
   "source": [
    "# Frequency Distribution WITHOUT stop words, non-alpha, punctuations\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(words_filtered)\n",
    "fdist.most_common(10)\n",
    "fdist.tabulate(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ce7da13-01cf-473f-af74-1241f5005d5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " freedom     ring    dream      let      day    every      one     able together   nation \n",
      "      13       12       11       11       10        9        8        8        7        4 \n"
     ]
    }
   ],
   "source": [
    "# Alternative to fdist=FreqDist(words)\n",
    "text = nltk.Text(words_filtered)\n",
    "fd = text.vocab()  # same as most_common()\n",
    "fd.tabulate(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51e811e5-dfac-4324-a0c1-50e6a39b3e28",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ea0f4d07f131>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobability\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfdistfull\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mfdistfull\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfdistfull\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \"\"\"\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     return [\n\u001b[1;32m    132\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \"\"\"\n\u001b[1;32m    107\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizers/punkt/{0}.pickle\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1272\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \"\"\"\n\u001b[0;32m-> 1274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1326\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m         \"\"\"\n\u001b[0;32m-> 1328\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1326\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m         \"\"\"\n\u001b[0;32m-> 1328\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1318\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1319\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1357\u001b[0m         \"\"\"\n\u001b[1;32m   1358\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1360\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1332\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1333\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"after_tok\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# Frequency Distribution WITH stop words, non-alpha, punctuations\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "fdistfull = FreqDist()\n",
    "for word in word_tokenize(text):\n",
    "    fdistfull[word] +=1\n",
    "fdistfull.most_common(10)\n",
    "fdistfull.tabulate(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4faffc79-85a6-4f24-b552-1dee3b1ca2da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist['dream']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce00a437-5275-4d64-bd84-bc2009468af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist['freedom']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fc8a4c-64f4-4471-bd17-a337e5ec8f3c",
   "metadata": {},
   "source": [
    "### Step 6: Check word combinations (parts of speech)\n",
    "<font color=blue> **Concordance:**</font> collection of word locations with their context <br>\n",
    "<font color=blue> **Collocation:**</font> series of words the frequently go together <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "768fb13d-261c-44ab-ac6c-bbca57e9cef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 11 matches:\n",
      " today and tomorrow , I still have a dream . It is a dream deeply rooted in the\n",
      "row , I still have a dream . It is a dream deeply rooted in the American dream \n",
      " dream deeply rooted in the American dream . I have a dream that one day this n\n",
      "ted in the American dream . I have a dream that one day this nation will rise u\n",
      "all men are created equal . I have a dream that one day on the red hills of Geo\n"
     ]
    }
   ],
   "source": [
    "# Concordance\n",
    "file = open('I have a dream excerpt.txt', 'rt')\n",
    "raw = file.read()\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "Text_Con = nltk.Text(tokens)\n",
    "Text_Con.concordance('dream', lines=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "532766d6-f7f0-428d-be78-8dd100cd15a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 4 of 4 matches:\n",
      " . I have a dream that one day this nation will rise up and live out the true \n",
      "tle children will one day live in a nation where they will not be judged by th\n",
      "nsform the jangling discords of our nation into a beautiful symphony of brothe\n",
      "g . And if America is to be a great nation , this must become true . And so le\n"
     ]
    }
   ],
   "source": [
    "Text_Con.concordance('nation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "798148d1-bc1c-42c0-a872-5b8cd2040f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " today and tomorrow , I still have a dream . It is a dream deeply rooted in the\n",
      "row , I still have a dream . It is a dream deeply rooted in the American dream \n",
      " dream deeply rooted in the American dream . I have a dream that one day this n\n",
      "ted in the American dream . I have a dream that one day this nation will rise u\n",
      "all men are created equal . I have a dream that one day on the red hills of Geo\n"
     ]
    }
   ],
   "source": [
    "concordance_list = Text_Con.concordance_list(\"dream\", lines=5)\n",
    "for entry in concordance_list:\n",
    "    print(entry.line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eeae9931-0308-4e09-8038-5970c0d3956c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('freedom', 'ring'), 11),\n",
       " (('let', 'freedom'), 10),\n",
       " (('one', 'day'), 8),\n",
       " (('dream', 'one'), 5),\n",
       " (('faith', 'able'), 3)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collocation for 2 most frequent words together\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(words_filtered)\n",
    "finder.ngram_fd.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d7df501-2fa2-4102-b052-2ff81addf5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('let', 'freedom', 'ring'), 10),\n",
       " (('dream', 'one', 'day'), 5),\n",
       " (('dream', 'today', 'dream'), 2),\n",
       " (('today', 'dream', 'one'), 2),\n",
       " (('able', 'join', 'hands'), 2)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collocation for 3 most frequent words together\n",
    "finder = nltk.collocations.TrigramCollocationFinder.from_words(words_filtered)\n",
    "finder.ngram_fd.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7cc45747-dd27-42ae-bc4b-fe3fc99606d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('dream', 'today', 'dream', 'one'), 2),\n",
       " (('today', 'dream', 'one', 'day'), 2),\n",
       " (('every', 'mountainside', 'let', 'freedom'), 2),\n",
       " (('mountainside', 'let', 'freedom', 'ring'), 2),\n",
       " (('even', 'though', 'face', 'difficulties'), 1)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collocation for 4 most frequent words together\n",
    "finder = nltk.collocations.QuadgramCollocationFinder.from_words(words_filtered)\n",
    "finder.ngram_fd.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310665a2-0aab-4250-be05-d05d72fae211",
   "metadata": {},
   "source": [
    "## Part 2 \n",
    "### Putting it all together (State of the Union, JFK 1961)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5b031b5-2190-4da3-8571-38b1980137fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to /Users/BGBlanco/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/BGBlanco/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     /Users/BGBlanco/nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/BGBlanco/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/BGBlanco/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/BGBlanco/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download([\n",
    "\"names\",\n",
    "\"stopwords\",\n",
    "\"state_union\",\n",
    "\"averaged_perceptron_tagger\",\n",
    "\"vader_lexicon\",\n",
    "\"punkt\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59616812-43be-46fe-a1ec-b959ea450bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Print Sentences ---------\n",
      "PRESIDENT JOHN F. KENNEDY'S SPECIAL MESSAGE TO THE CONGRESS ON URGENT NATIONAL NEEDS\n",
      " \n",
      "May 25, 1961\n",
      "\n",
      "Mr. Speaker, Mr. Vice President, my copartners in Government, gentlemen-and ladies:\n",
      "The Constitution imposes upon me the obligation to \"from time to time give to the Congress information of the State of the Union.\"\n",
      "--------- Print lowercase word tokens ---------\n",
      "['president', 'john', 'f.', 'kennedy', \"'s\", 'special', 'message', 'to', 'the', 'congress', 'on', 'urgent', 'national', 'needs', 'may', '25', ',', '1961', 'mr.', 'speaker', ',', 'mr.', 'vice', 'president', ',', 'my', 'copartners', 'in', 'government', ',', 'gentlemen-and', 'ladies', ':', 'the', 'constitution', 'imposes', 'upon', 'me', 'the', 'obligation']\n",
      "--------- Remove Punctuations ---------\n",
      "['president', 'john', 'f', 'kennedy', 's', 'special', 'message', 'to', 'the', 'congress', 'on', 'urgent', 'national', 'needs', 'may', '25', '', '1961', 'mr', 'speaker', '', 'mr', 'vice', 'president', '', 'my', 'copartners', 'in', 'government', '', 'gentlemenand', 'ladies', '', 'the', 'constitution', 'imposes', 'upon', 'me', 'the', 'obligation']\n",
      "--------- Remove Non-alpha characters ---------\n",
      "['president', 'john', 'f', 'kennedy', 's', 'special', 'message', 'to', 'the', 'congress', 'on', 'urgent', 'national', 'needs', 'may', 'mr', 'speaker', 'mr', 'vice', 'president', 'my', 'copartners', 'in', 'government', 'gentlemenand', 'ladies', 'the', 'constitution', 'imposes', 'upon', 'me', 'the', 'obligation', 'to', 'from', 'time', 'to', 'time', 'give', 'to']\n",
      "--------- Print without stopwords ---------\n",
      "['president', 'john', 'f', 'kennedy', 'special', 'message', 'congress', 'urgent', 'national', 'needs', 'may', 'mr', 'speaker', 'mr', 'vice', 'president', 'copartners', 'government', 'gentlemenand', 'ladies', 'constitution', 'imposes', 'upon', 'obligation', 'time', 'time', 'give', 'congress', 'information', 'state', 'union', 'traditionally', 'interpreted', 'annual', 'affair', 'tradition', 'broken', 'extraordinary', 'times', 'extraordinary']\n",
      "--------- Most Common Words ---------\n",
      "     new congress  freedom     must  defense \n",
      "      31       24       22       21       20 \n",
      "--------- Concordance ---------\n",
      "Displaying 5 of 22 matches:\n",
      "n this nation the role of leader in freedom 's cause . No role in history could\n",
      "lt or more important . We stand for freedom . That is our conviction for oursel\n",
      "ystem -- except as it is hostile to freedom . Nor am I here to present a new mi\n",
      "one area . I am here to promote the freedom doctrine . I . The great battlegrou\n",
      "nd for the defense and expansion of freedom today is the whole southern half of\n",
      "--------- Collocation ---------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('to', 'the', 'congress'), 7),\n",
       " (('we', 'can', 'not'), 6),\n",
       " (('as', 'well', 'as'), 5),\n",
       " (('of', 'the', 'congress'), 5),\n",
       " (('an', 'additional', 'million'), 5)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "state_union.fileids()\n",
    "\n",
    "#load text\n",
    "Kennedy = state_union.raw('1961-Kennedy.txt')\n",
    "\n",
    "# split into sentences\n",
    "from nltk import sent_tokenize\n",
    "Kennedy_sentences = sent_tokenize(Kennedy)\n",
    "print('--------- Print Sentences ---------')\n",
    "print(Kennedy_sentences[0])  # print 1st sentence with [0] \n",
    "\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "Kennedy_tokens = word_tokenize(Kennedy)\n",
    "\n",
    "# convert to lower case\n",
    "Kennedy_tokens = [w.lower()for w in Kennedy_tokens]\n",
    "\n",
    "print('--------- Print lowercase word tokens ---------')\n",
    "print(Kennedy_tokens[:40])\n",
    "\n",
    "# remove punctuations\n",
    "import string\n",
    "table = str.maketrans('','',string.punctuation)\n",
    "Kennedy_no_punct = [w.translate(table) for w in Kennedy_tokens]\n",
    "print('--------- Remove Punctuations ---------')\n",
    "print(Kennedy_no_punct[:40])\n",
    "\n",
    "# remove non-alpha words\n",
    "Kennedy_words = [word for word in Kennedy_no_punct if word.isalpha()]\n",
    "print('--------- Remove Non-alpha characters ---------')\n",
    "print(Kennedy_words[:40])\n",
    "\n",
    "# filter out stop words\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "Kennedy_Final = [w for w in Kennedy_words if not w in stopwords]\n",
    "print('--------- Print without stopwords ---------')\n",
    "print(Kennedy_Final[:40])\n",
    "\n",
    "# Frequency Distribution WITHOUT stop words, non-alpha, punctuations\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(Kennedy_Final)\n",
    "print('--------- Most Common Words ---------')\n",
    "fdist.most_common(5)\n",
    "fdist.tabulate(5)\n",
    "\n",
    "# Concordance\n",
    "tokens = nltk.word_tokenize(Kennedy)\n",
    "Kennedy_Con = nltk.Text(tokens)\n",
    "print('--------- Concordance ---------')\n",
    "Kennedy_Con.concordance('freedom', lines=5)\n",
    "\n",
    "# Collocation for 3 most frequent words together\n",
    "finder = nltk.collocations.TrigramCollocationFinder.from_words(Kennedy_words)\n",
    "print('--------- Collocation ---------')\n",
    "finder.ngram_fd.most_common(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bbae06-5f78-4451-b0ec-d899f630ebef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
