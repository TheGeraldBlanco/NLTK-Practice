{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b834d247-bbc5-4066-8710-516358dbd0d2",
   "metadata": {},
   "source": [
    "## Part 3 \n",
    "### NLTK Sentiment Analyzer - Twitter Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "1fd35632-0cbc-43cd-a091-2568dcd44e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/BGBlanco/opt/anaconda3/lib/python3.8/site-packages (3.6.1)\n",
      "Requirement already satisfied: tqdm in /Users/BGBlanco/opt/anaconda3/lib/python3.8/site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: regex in /Users/BGBlanco/opt/anaconda3/lib/python3.8/site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: click in /Users/BGBlanco/opt/anaconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in /Users/BGBlanco/opt/anaconda3/lib/python3.8/site-packages (from nltk) (1.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to /Users/BGBlanco/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/BGBlanco/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/BGBlanco/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/BGBlanco/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/BGBlanco/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/BGBlanco/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download([\n",
    "\"names\",\n",
    "\"stopwords\",\n",
    "\"twitter_samples\",\n",
    "\"averaged_perceptron_tagger\",\n",
    "\"vader_lexicon\",\n",
    "\"punkt\",\n",
    "])\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "twitter_samples.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a3768498-ee1f-433a-ac65-d59588d2f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load with inactive URLs\n",
    "positive_tweets = [t.replace(\"://\", \"//\") for t in nltk.corpus.twitter_samples.strings('positive_tweets.json')]\n",
    "negative_tweets = [t.replace(\"://\", \"//\") for t in nltk.corpus.twitter_samples.strings('negative_tweets.json')]\n",
    "all_tweets = [t.replace(\"://\", \"//\") for t in nltk.corpus.twitter_samples.strings('tweets.20150430-223406.json')]\n",
    "combined_tweets = positive_tweets + negative_tweets + all_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "76376522-1781-4953-9e61-15d62832fe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load with active URLs\n",
    "pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "neg_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "posneg_tweets = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "all_posneg_tweets = pos_tweets + neg_tweets + neu_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2edafe-cb40-4d7c-abde-5fb53ec2788a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: red;\">\n",
    " <p style=\"color: white\">\n",
    "  ** CLEANING TWEETS **\n",
    " </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd09f69d-4fb2-4a92-a94f-b9c664a24223",
   "metadata": {},
   "source": [
    "#### Assigning special characters like <span style='background :yellow' > emojis </span> \n",
    "- set happy and sad emojis <br>\n",
    "- combine with **Union** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "062dd0c6-88de-46bd-862b-aee5f0936702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "stopwords_english = stopwords.words('english')\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# happy emojis\n",
    "happy_emoji = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    "\n",
    "# Sad emojis\n",
    "sad_emoji = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "\n",
    "# all emojis (happy + sad)\n",
    "emojis = happy_emoji.union(sad_emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e828d31-0fa2-4c27-8a98-6f4c90c3dde1",
   "metadata": {},
   "source": [
    "#### Cleaning Tweets\n",
    "- <font color=blue> **def**</font> - define a function <br>\n",
    "- <font color=blue> **re.sub()**</font> - replace patterns in strings <br>\n",
    "- <font color=blue> **str()**</font> - to avoid error if function expects integer or float format <br>\n",
    "    - tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet) --> if function doesn't require string\n",
    "\n",
    "#### Tokenizing Tweets\n",
    "- **TweetTokenizer** - module used to tokenize tweets <br>\n",
    "- <font color=blue> **preserve_case**</font> if False then converts tweet to lowercase <br>\n",
    "- <font color=blue> **strip_handles**</font> if True then removes twitter handles from the tweet <br>\n",
    "- <font color=blue> **reduce_len**</font> if True then reduce length of words in the a tweet like 'yaaaahooooo' <br>\n",
    "\n",
    "\n",
    "#### Items to clean\n",
    "- stock tickers ($APPL), retweet (RT), hyperlinks, hashtags <br>\n",
    "- stopwords, emojis, punctuations, stem words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "cc4ced37-84b4-4ead-9569-201fece3cbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_it(tweet):\n",
    "    # remove stock tickers $APPL\n",
    "    tweet = re.sub(r'\\$\\w*', '', str(tweet))\n",
    "    \n",
    "    # remove retweet \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', str(tweet))\n",
    "\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*â™›', '', str(tweet))\n",
    "\n",
    "     # remove more hyperlinks\n",
    "    tweet = re.sub(r'http.', '', str(tweet))\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', str(tweet))\n",
    "\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and # remove stopwords\n",
    "              word not in emojis and # remove emoticons\n",
    "                word not in string.punctuation): # remove punctuation\n",
    "            #tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word) # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "ca370663-5a26-4bf5-aed6-0645e6b4b28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Test Tweet ---------\n",
      "['religion', 'end', 'philosophi', 'begin', 'alchemi', 'end', 'chemistri', 'begin', 'astrolog', 'end', 'astronomi', 'begin', 'great', 'christoph', 'hitchen', 'enlighten', ':/', 'christopherhitchens.net/']\n",
      "--------- Uncleaned Tweet ---------\n",
      "@BhaktisBanter @PallaviRuhail This one is irresistible :)\n",
      "#FlipkartFashionFriday http//t.co/EbZ0L2VENM\n",
      "--------- Cleaned Tweet ---------\n",
      "['one', 'irresist', 'flipkartfashionfriday', 't.co/ebz0l2venm']\n"
     ]
    }
   ],
   "source": [
    "Test_Tweet = \"\"\"RT @geraldblanco13 Religion ends where philosophy begins, just as alchemy ends where \n",
    "chemistry begins, and astrology ends where astronomy begins. ~ from the great Christopher Hitchens! \n",
    "#enlightened :) https://christopherhitchens.net/\"\"\"\n",
    "\n",
    "# print cleaned tweet\n",
    "\n",
    "print('--------- Test Tweet ---------')\n",
    "print (clean_it(Test_Tweet))\n",
    "\n",
    "print('--------- Uncleaned Tweet ---------')\n",
    "print (positive_tweets[5])\n",
    "\n",
    "print('--------- Cleaned Tweet ---------')\n",
    "print (clean_it(positive_tweets[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "62341142-a450-4479-a5a6-ac73cd5caf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_pos_tweets = clean_it(positive_tweets)\n",
    "clean_neg_tweets = clean_it(negative_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ca0202-75c4-4e86-8c5e-2e6ec976cdd2",
   "metadata": {},
   "source": [
    "#### Feature Extraction\n",
    "- <font color=blue> **dict**</font> - creates an unordered, changeable, and indexed dictionary <br>\n",
    "- <font color=blue> **append()**</font> - add items to end of the list <br>\n",
    "- <font color=blue> **return**</font> - return value from created function <br>\n",
    "\n",
    "- define function on bag_of_words to extract unigram features from tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "bfd57e0c-545f-4d0c-938e-b87533257364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'religion': True, 'end': True, 'philosophi': True, 'begin': True, 'alchemi': True, 'chemistri': True, 'astrolog': True, 'astronomi': True, 'great': True, 'christoph': True, 'hitchen': True, 'enlighten': True, ':/': True, 'christopherhitchens.net/': True}\n"
     ]
    }
   ],
   "source": [
    "# feature extractor function\n",
    "def bag_of_words(tweet):\n",
    "    words = clean_it(tweet)\n",
    "    words_dictionary = dict([word, True] for word in words)\t \n",
    "    return words_dictionary\n",
    "\n",
    "print (bag_of_words(Test_Tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "f1a2ea02-4876-4aef-a7fc-37877b08b8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5000\n"
     ]
    }
   ],
   "source": [
    "# positive tweets feature set\n",
    "positive_tweets_set = []\n",
    "for tweet in positive_tweets:\n",
    "    positive_tweets_set.append((bag_of_words(tweet), 'pos'))\n",
    "\n",
    "# negative tweets feature set\n",
    "negative_tweets_set = []\n",
    "for tweet in negative_tweets:\n",
    "    negative_tweets_set.append((bag_of_words(tweet), 'neg'))\n",
    "\n",
    "print (len(positive_tweets_set), len(negative_tweets_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f7a2bd-dcc8-4977-9673-ae5d8c214835",
   "metadata": {},
   "source": [
    "#### Train and Test Set\n",
    "- <font color=blue> **Test Set**</font> - [:1000] for 1000 samples <br>\n",
    "- <font color=blue> **Train Set**</font> - [1000:] means remaining samples after deducting 1000 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "45141d55-3298-40ba-9732-d1cfbdedb38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 8000\n"
     ]
    }
   ],
   "source": [
    "# randomize positive_tweet_set and negative_tweets_set\n",
    "\n",
    "from random import shuffle \n",
    "shuffle(positive_tweets_set)\n",
    "shuffle(negative_tweets_set)\n",
    "\n",
    "test_set = positive_tweets_set[:1000] + negative_tweets_set[:1000]\n",
    "train_set = positive_tweets_set[1000:] + negative_tweets_set[1000:]\n",
    "\n",
    "print(len(test_set),  len(train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a4e61c-813f-45ce-a91d-2909235b1b53",
   "metadata": {},
   "source": [
    "#### Classifier Training and Accuracy  \n",
    "<font color=blue> **NaiveBayesClassifier**</font> - paramaterized by 2 probability distribution <br>\n",
    "- P(label) probablity that an input will receive each label, given no info about its features <br>\n",
    "- P(fname=fval|label) probability that a given feature (fname) will receive a given value (fval), given its label\n",
    "\n",
    "References:\n",
    "- https://www.nltk.org/book/ch06.html\n",
    "- https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "8c4eaa7a-f3b4-40a9-8230-1f2b1ac6ecdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7435\n",
      "Most Informative Features\n",
      "                     bam = True              pos : neg    =     25.7 : 1.0\n",
      "                     sad = True              neg : pos    =     20.8 : 1.0\n",
      "                     x15 = True              neg : pos    =     20.3 : 1.0\n",
      "                 appreci = True              pos : neg    =     17.7 : 1.0\n",
      "                 retweet = True              pos : neg    =     15.0 : 1.0\n",
      "                     ugh = True              neg : pos    =     15.0 : 1.0\n",
      "                    glad = True              pos : neg    =     13.4 : 1.0\n",
      "                 congrat = True              pos : neg    =     13.0 : 1.0\n",
      "                 definit = True              pos : neg    =     13.0 : 1.0\n",
      "                     via = True              pos : neg    =     13.0 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "accuracy = classify.accuracy(classifier, test_set)\n",
    "\n",
    "print(accuracy)\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcabe4f8-6dc7-4c24-8f72-3de2d85f106a",
   "metadata": {},
   "source": [
    "<font color=blue> **How to Interpret** </font> NaiveBayesClassifier <br>\n",
    "- **'show_most_informative_features()'** specifies the top features that are most effective in classifying pos or neg\n",
    "-  **0.7435** - the % of tweets correctly classified\n",
    "- Example\n",
    "> - **'sad'** in the training set is TRUE (it's in the bag_of_words)\n",
    "> - **'sad'** appeared in 'neg' or negative_tweets 20.8 times that in 'pos' or positive_tweets\n",
    "\n",
    "<font color=red>**The sets are randomly shuffled, so results will change**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967c783b-8e32-484f-8ad5-cec5b1d1c55f",
   "metadata": {},
   "source": [
    "#### Testing Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "3241c0fb-0656-412b-b880-8da2342d92be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n"
     ]
    }
   ],
   "source": [
    "# Can the classifier correctly classify this negative tweet?\n",
    "test_tweet = \"@JustinTrudeau when are you going to crucify the b@astards who killed all those children?!?\"\n",
    "test_tweet_set = bag_of_words(test_tweet)\n",
    "print (classifier.classify(test_tweet_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "1db7bc55-f62a-486b-b907-efcffa64ba4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n"
     ]
    }
   ],
   "source": [
    "# Can the classifier correctly classify this positive tweet?\n",
    "test_tweet = \"Happy Canada day y'all!!!\"\n",
    "test_tweet_set = bag_of_words(test_tweet)\n",
    "print (classifier.classify(test_tweet_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc54d040-ffc0-4bc8-93e4-8ae31c3dd092",
   "metadata": {},
   "source": [
    "### scikit-learn Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "8eafbec6-575a-4ecc-8998-916ef55c97c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/BGBlanco/opt/anaconda3/lib/python3.8/site-packages (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/BGBlanco/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (1.20.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/BGBlanco/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (1.6.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/BGBlanco/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/BGBlanco/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "from sklearn.naive_bayes import (\n",
    "    BernoulliNB,\n",
    "    ComplementNB,\n",
    "    MultinomialNB,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "ace6d326-b636-46e3-9889-1fc46bda4548",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"BernoulliNB\": BernoulliNB(),\n",
    "    \"ComplementNB\": ComplementNB(),\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "    \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(),\n",
    "    \"MLPClassifier\": MLPClassifier(max_iter=1000),\n",
    "    \"AdaBoostClassifier\": AdaBoostClassifier(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "06de1135-8261-4b35-9d10-88b25adcb82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.90% - BernoulliNB\n",
      "74.25% - ComplementNB\n",
      "74.25% - MultinomialNB\n",
      "65.45% - KNeighborsClassifier\n",
      "68.90% - DecisionTreeClassifier\n",
      "72.20% - RandomForestClassifier\n",
      "73.50% - LogisticRegression\n",
      "68.85% - MLPClassifier\n",
      "67.15% - AdaBoostClassifier\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle \n",
    "shuffle(positive_tweets_set)\n",
    "shuffle(negative_tweets_set)\n",
    "\n",
    "for name, sklearn_classifier in classifiers.items():\n",
    "         classifier = nltk.classify.SklearnClassifier(sklearn_classifier)\n",
    "         classifier.train(train_set)\n",
    "         accuracy = nltk.classify.accuracy(classifier, test_set)\n",
    "         print(F\"{accuracy:.2%} - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12100c9-3dcd-4d22-be53-e9aef0b72804",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d811f782-1df5-4b8e-9993-b31e2378ed26",
   "metadata": {},
   "source": [
    "### Precision, Recall, F1-Score, Confusion Matrix\n",
    "<font color=red>**EXPLORE FURTHER**</font>\n",
    "https://blog.chapagain.com.np/python-nltk-twitter-sentiment-analysis-natural-language-processing-nlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "db5d4d7d-39c5-4426-8fd7-8886dc34d556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision - Positive: 0.7321258341277407\n",
      "Recall - Positive: 0.768\n",
      "F-measure - Positive: 0.7496339677891655\n",
      "Precision - Negative: 0.7560462670872765\n",
      "Recall - Negative: 0.719\n",
      "F-measure - Negative: 0.7370579190158891\n",
      "    |   n   p |\n",
      "    |   e   o |\n",
      "    |   g   s |\n",
      "----+---------+\n",
      "neg |<719>281 |\n",
      "pos | 232<768>|\n",
      "----+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "actual_set = defaultdict(set)\n",
    "predicted_set = defaultdict(set)\n",
    "\n",
    "actual_set_cm = []\n",
    "predicted_set_cm = []\n",
    "\n",
    "for index, (feature, actual_label) in enumerate(test_set):\n",
    "    actual_set[actual_label].add(index)\n",
    "    actual_set_cm.append(actual_label)\n",
    "\n",
    "    predicted_label = classifier.classify(feature)\n",
    "\n",
    "    predicted_set[predicted_label].add(index)\n",
    "    predicted_set_cm.append(predicted_label)\n",
    "    \n",
    "from nltk.metrics import precision, recall, f_measure, ConfusionMatrix\n",
    "\n",
    "print ('Precision - Positive:', precision(actual_set['pos'], predicted_set['pos']))\n",
    "print ('Recall - Positive:', recall(actual_set['pos'], predicted_set['pos']))  \n",
    "print ('F-measure - Positive:', f_measure(actual_set['pos'], predicted_set['pos']))  \n",
    "\n",
    "print ('Precision - Negative:', precision(actual_set['neg'], predicted_set['neg']))  \n",
    "print ('Recall - Negative:', recall(actual_set['neg'], predicted_set['neg']))  \n",
    "print ('F-measure - Negative:', f_measure(actual_set['neg'], predicted_set['neg']))\n",
    "\n",
    "cm = ConfusionMatrix(actual_set_cm, predicted_set_cm)\n",
    "print (cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a72876d-8b18-48b9-a4ed-2e8477f5871d",
   "metadata": {},
   "source": [
    "### Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "40e561df-22ee-477b-bc4a-c173f6bccb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_fd = nltk.FreqDist(clean_pos_tweets)\n",
    "negative_fd = nltk.FreqDist(clean_neg_tweets)\n",
    "common_set = set(positive_fd).intersection(negative_fd)\n",
    "\n",
    "for word in common_set:\n",
    "    del positive_fd[word]\n",
    "    del negative_fd[word]\n",
    "    \n",
    "top_100_pos = {word for word, count in positive_fd.most_common(100)}\n",
    "top_100_neg = {word for word, count in negative_fd.most_common(100)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99592aac-9e79-48d7-a533-ebb0082be602",
   "metadata": {},
   "source": [
    "### Setting up bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "0202374b-1f66-47ad-9fde-632cfb871670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Positive Collocation ---------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('new', 'follow'), 65),\n",
       " (('follow', 't.co/rcvcyyo0iq'), 62),\n",
       " (('t.co/rcvcyyo0iq', 'follow'), 62),\n",
       " (('follow', 'u'), 62),\n",
       " (('u', 'back'), 62)]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up positive and negative bigram finders\n",
    "\n",
    "positive_bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(clean_pos_tweets)\n",
    "negative_bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(clean_neg_tweets)\n",
    "print('--------- Positive Collocation ---------')\n",
    "positive_bigram_finder.ngram_fd.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "dbabd27f-2380-400e-8fc7-5a983bd35931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Negative Collocation ---------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('â™›', 'â™›'), 141),\n",
       " (('ã€‹', 'ã€‹'), 140),\n",
       " (('n', 'ã€‹'), 70),\n",
       " (('pleas', 'follow'), 61),\n",
       " (('thank', 'n'), 52)]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('--------- Negative Collocation ---------')\n",
    "negative_bigram_finder.ngram_fd.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14851c6-90b1-4123-88f2-a787458de2cc",
   "metadata": {},
   "source": [
    "### Part of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36be316f-b240-4cc6-b4e2-c4a67037d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def is_positive(tweets: str) -> bool:\n",
    "    \"\"\"True if tweet has positive compound sentiment, False otherwise.\"\"\"\n",
    "    text = nltk.corpus.twitter_samples.raw(tweets)\n",
    "    scores = [\n",
    "        sia.polarity_scores(sentence)[\"compound\"]\n",
    "        for sentence in nltk.sent_tokenize(text)\n",
    "    ]\n",
    "    return mean(scores) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5bdb9546-a40c-47b7-b705-d7a7014c02f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FollowFriday\n",
      "@France_Inte\n",
      "@PKuchly57\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "test_pos = twitter_samples.tokenized('positive_tweets.json')[0] #1st string\n",
    "print(test_pos[0]) # first word\n",
    "print(test_pos[1]) # 2nd word\n",
    "print(test_pos[2]) # 3rd word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4c739391-4fea-4599-9a9c-38a540b5d217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Lamb2ja\n",
      "Hey\n",
      "James\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "test_pos = twitter_samples.tokenized('positive_tweets.json')[1] #2nd string\n",
    "print(test_pos[0]) # first word\n",
    "print(test_pos[1]) # 2nd word\n",
    "print(test_pos[2]) # 3rd word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "13c02f6c-c05f-4264-9149-484022042249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n",
      "['@Lamb2ja', 'Hey', 'James', '!', 'How', 'odd', ':/', 'Please', 'call', 'our', 'Contact', 'Centre', 'on', '02392441234', 'and', 'we', 'will', 'be', 'able', 'to', 'assist', 'you', ':)', 'Many', 'thanks', '!']\n",
      "['@DespiteOfficial', 'we', 'had', 'a', 'listen', 'last', 'night', ':)', 'As', 'You', 'Bleed', 'is', 'an', 'amazing', 'track', '.', 'When', 'are', 'you', 'in', 'Scotland', '?', '!']\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "test_pos = twitter_samples.tokenized('positive_tweets.json') # no specified string sets\n",
    "print(test_pos[0]) # 1st tweet\n",
    "print(test_pos[1]) # 2nd tweet\n",
    "print(test_pos[2]) # 3rd tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b8d3dd45-56e8-4e6b-a122-3015ce30abbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/BGBlanco/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet') # determines the base word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d6599192-69b0-4f46-b387-d421178af5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#FollowFriday', 'JJ'), ('@France_Inte', 'NNP'), ('@PKuchly57', 'NNP'), ('@Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':)', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Part of Speech Tag\n",
    "from nltk.tag import pos_tag\n",
    "print(pos_tag(test_pos[0]))\n",
    "\n",
    "# JJ - adjective; NNP - proper noun; IN - preposition or conjunction; VBG - verb, part participle; \n",
    "# NNS - plural noun; NN - singular noun; PRP$ - possive noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bc704453-6ffc-46dd-a3fc-d0f7adc61101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> False Ed Miliband is a tool\n",
      "> True @dylanobrien @MazeRunnerMovie IT IS JUST BEYOND WORDS, sooooooo freaking good! I'm going to die before it comes out!! :D\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "def is_positive(tweets: str) -> bool:\n",
    "    \"\"\"True if tweet has positive compound sentiment, False otherwise.\"\"\"\n",
    "    return sia.polarity_scores(tweets)[\"compound\"] > 0\n",
    "\n",
    "shuffle(combined_tweets)\n",
    "for tweets in combined_tweets[:2]:\n",
    "    print(\">\", is_positive(tweets), tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "936e1ce7-f308-44b7-8d2e-330d48833f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def is_positive(tweets: str) -> bool:\n",
    "    \"\"\"True if tweet has positive compound sentiment, False otherwise.\"\"\"\n",
    "    text = nltk.corpus.twitter_samples.raw(tweets)\n",
    "    scores = [\n",
    "        sia.polarity_scores(sentence)[\"compound\"]\n",
    "        for sentence in nltk.sent_tokenize(text)\n",
    "    ]\n",
    "    return mean(scores) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "58f6c03e-f0b7-449b-bfed-6bf559d89634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.56% correct\n"
     ]
    }
   ],
   "source": [
    "shuffle(combined_tweets)\n",
    "correct = 0\n",
    "\n",
    "for tweets in combined_tweets:\n",
    "    if is_positive(tweets):\n",
    "        if tweets in positive_tweets:\n",
    "            correct +=1\n",
    "    else:\n",
    "        if tweets in negative_tweets:\n",
    "            correct +=1\n",
    "            \n",
    "print(F\"{correct / len(combined_tweets):.2%} correct\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
